# Les modèles génératifs 

En exemple de ce genre de modèle on trouve le désormais GPT 3.5 (ou ChatGPT) d'OpenAI. Je propose d'utiliser les modèles de MistralAI, une entreprise française avec une approche favorable à la science ouverte (on peut affiner le modèle et le télécharger localement à condition d'avoir les ressources nécessaires pour faire tourner ces modèles). Il existe de nombreux modèles de ce type et GPT 3.5 n'est pas le premier comme l'indique son nom. Certains modèles sont suffisamment petits pour pouvoir être affinés avec un CPU et téléchargé localement (un modèle comme [Flan-T5](https://huggingface.co/google/flan-t5-base) de Google se prête bien à une utilisation de ce genre mais attention à la fenêtre contextuelle de (512 tokens)) 


Contrairement aux modèles précédemment cités les modèles génératifs sont des modèles prédictifs qui ont la particularité de générer des données à partir d'une instruction. 

Cette instruction est nommée un _prompt_ et elle est rédigée en langage naturel, càd qu'elle ne nécessite pas de préparation des données à proprement parler. 

Mais comme pour tousles modèles, ils ont aussi une "fenêtre contextuelle" limitée qu'il faut pouvoir anticiper afin de s'assurer que l'intégralité de nos données sont bien traitées. 

Les grand modèles comme Mistral-7b ont des fenêtreS contextuelles dépassant 30 000 tokens (1 mot équivaut à 1 à 4 tokens, on compte souvent 1,50 pour calculer approximativement le nombre de tokens d'un texte, mais cela dépend notamment de la langue). Et ces fenêtres sont amenées à s'élargir dans les prochains temps. Il faut pourtant garder en tête que plus une instruction est longue plus il devient compliqué pour un modèle de "se rappeler" de l'instruction de départ. Cela signifie que le modèle tend à générer des réponses que l'on considère comme incorrectes en fonction de la longueur du prompt en entrée. 

Ces réponses incorrectes sont aussi nommées des hallucinations : cela prend plusieurs formes. Le modèle peut générer un texte qui est en lien avec le prompt mais ne répond pas à la question ou l'instruction explicite (à "Extrait les entités nommées de la phrase suivante "Paris est la capitale de la France."", le modèle génère : "Extrait les étiquettes morpho-syntaxiques de la phrase suivante : "Berlin est la capitale de l'Allemagne"". ) ou "improviser" des éléments de réponse (ex : "Extrait les EN en fonction des étiquettes "PER", "LOC" " le modèle génère des étiquettes "MISC" qui ne font pourtant pas partie des étiquettes proposées). 

Autre problème : tous les LLMs ont une varible aléatoire. Le processus par lequel une donnée en langue naturelle est encodée dans la modélisation complète de la langue comprend une variable aléatoire ce qui signifie qu'un même prompt avec un même texte est susceptible de générer des réponses différentes. Une façon de limiter cet effet est de contrôler la température d'un modèle. La température de 0 à 1 avec 0 "froid" et 1 "chaud" mesure le fait qu'un modèle soit plus "déterministe" ou "créatif", càd sa propension à ajouter des valeurs plus aléatoires et éloignées de la sortie statistiquement la plus probable. C'est un des paramètres que l'on peut contrôler via l'API mais pas sur l'interface web proposée par les entreprises. 

## Division en batchs


Afin que l'intégralité de nos données puissent être traitées il faut diviser le texte en prenant en compte la fenêtre contextuelle. Une fenêtre efficace est autour de 3 000 tokens, au delà le modèle tend à halluciner davantage. 

Le programme [generative_models_ner.py](./../generative_models_ner.py) permet de diviser un texte ou une série de pages txt en batches d'une longueur donnée en tenant en compte l'instruction de départ. Voir paramètres. 

## Prompt engineering

Le "prompt engineering" consiste à construire la meilleure instruction possible pour une tâche demandée. De nombreux modèles ou "template" existent aujourd'hui pour exploiter au mieux les capacités des LLMs (notamment leurs capacités de "reflexion"). _Chain of Thought_, _Structured output_, _in context learning_ sont parmi les plus utilisées. 

_chain of thought_ (CoT) demande au modèle de générer des raisons pour chacune de ses réponses, ce qui est utile quand le prompt est complexe (résoudre un puzzle, utiliser des fonctions particulières sur la base d'outils). Pas utile pour le NER qui est une tâche "connue".

_in context learning_: consiste à ajouter des exemples pour contextualiser la demande et cadrer les instructions. Pertinent pour nos données où certains noms propres ne doivent pas être sélectionnés (noms de référentiels), càd qu'un ou deux exemples peut enseigner au modèle d'ignorer les passages après l'adresse bibliographique.

## Utilisation de l'API

Le programme ```generative_models_ner.py``` permet soit en mode "manuel" de ne faire que écrire les batches dans des documents TXT qu'il faut ensuite manuellement c/c dans son interface web. Soit en mode "api", de faire seul le requêtage du modèle. 

Dans les deux cas il est possible de limiter le nombre de tokens maximum par batch, et je recommande 3000 tokens max. 

La sortie dans le cas de la requête api est un document csv qui contient du batch et la réponse générée (les entités nommées). 

Je ne propose pas de post traitement car la sortie dépend du template : si la réponse est une sortie en .json le traitemen est bien différent d'une liste ou d'un format déjà csv, ce qu'il est possible de demander. 